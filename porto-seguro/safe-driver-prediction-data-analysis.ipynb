{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar as bibliotecas necessÃ¡rias para este projeto\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train = pd.read_csv('../../data/train.csv')\n",
    "    test = pd.read_csv('../../data/test.csv')\n",
    "except e:\n",
    "    print(\"Error on trying read train dataset.\")\n",
    "finally:\n",
    "    print(\"dataset loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at how many rows and columns the train dataset contains\n",
    "rows = train.shape[0]\n",
    "columns = train.shape[1]\n",
    "print(\"The train dataset contains {0} rows and {1} columns\".format(rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(train.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "series = pd.Series([train['target'].sum(), len(train.values)], index=['1', '0'], name='train')\n",
    "series.plot.pie(figsize=(7, 7), autopct='%.2f', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 variables of grouping ind\n",
      "There are 3 variables of grouping reg\n",
      "There are 16 variables of grouping car\n",
      "There are 20 variables of grouping calc\n",
      "\n",
      "\n",
      "There are 17 binary variables\n",
      "There are 14 categorical variables\n",
      "There are 28 ordinal/numerical variables\n",
      "\n",
      "\n",
      "So later on we can create dummy variables for the 14 categorical variables.\n",
      "The ordinal/numerical variables we can use as such and the bin variables are already binary.\n"
     ]
    }
   ],
   "source": [
    "ind_vars = [] \n",
    "reg_vars = []\n",
    "car_vars = []\n",
    "calc_vars = []\n",
    "rest_vars = []\n",
    "\n",
    "bin_vars = []\n",
    "cat_vars = []\n",
    "num_ord_vars = []\n",
    "\n",
    "for f in train.columns:\n",
    "    if 'ind' in f:\n",
    "        ind_vars.append(f)\n",
    "    elif 'reg' in f:\n",
    "        reg_vars.append(f)\n",
    "    elif 'car' in f:\n",
    "        car_vars.append(f)\n",
    "    elif 'calc' in f:\n",
    "        calc_vars.append(f)\n",
    "    else:\n",
    "        rest_vars.append(f)\n",
    "        \n",
    "    if 'bin' in f:\n",
    "        bin_vars.append(f)\n",
    "    elif 'cat' in f:\n",
    "        cat_vars.append(f)\n",
    "    else:\n",
    "        num_ord_vars.append(f)\n",
    "        \n",
    "print('There are {} variables of grouping ind'.format(len(ind_vars)))\n",
    "print('There are {} variables of grouping reg'.format(len(reg_vars)))\n",
    "print('There are {} variables of grouping car'.format(len(car_vars)))\n",
    "print('There are {} variables of grouping calc'.format(len(calc_vars)))\n",
    "print('\\n')\n",
    "print('There are {} binary variables'.format(len(bin_vars)))\n",
    "print('There are {} categorical variables'.format(len(cat_vars)))\n",
    "print('There are {} ordinal/numerical variables'.format(len(num_ord_vars)))\n",
    "print(\"\\n\")\n",
    "print(\"So later on we can create dummy variables for the 14 categorical variables.\")\n",
    "print(\"The ordinal/numerical variables we can use as such and the bin variables are already binary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "train_copy = train\n",
    "train_copy = train_copy.replace(-1, np.NaN)\n",
    "\n",
    "# any() applied twice to check run the isnull check across all columns.\n",
    "train_copy.isnull().any().any()\n",
    "msno.bar(train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing = []\n",
    "\n",
    "for f in train.columns:\n",
    "    missings = train[train[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_06_bin\n",
      "[0 1]\n",
      "ps_ind_07_bin\n",
      "[1 0]\n",
      "ps_ind_08_bin\n",
      "[0 1]\n",
      "ps_ind_09_bin\n",
      "[0 1]\n",
      "ps_ind_10_bin\n",
      "[0 1]\n",
      "ps_ind_11_bin\n",
      "[0 1]\n",
      "ps_ind_12_bin\n",
      "[0 1]\n",
      "ps_ind_13_bin\n",
      "[0 1]\n",
      "ps_ind_16_bin\n",
      "[0 1]\n",
      "ps_ind_17_bin\n",
      "[1 0]\n",
      "ps_ind_18_bin\n",
      "[0 1]\n",
      "ps_calc_15_bin\n",
      "[0 1]\n",
      "ps_calc_16_bin\n",
      "[1 0]\n",
      "ps_calc_17_bin\n",
      "[1 0]\n",
      "ps_calc_18_bin\n",
      "[0 1]\n",
      "ps_calc_19_bin\n",
      "[0 1]\n",
      "ps_calc_20_bin\n",
      "[1 0]\n",
      "['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin']\n"
     ]
    }
   ],
   "source": [
    "cols_to_delete = []\n",
    "th = 0.1\n",
    "for col in range(0, len(bin_vars)):\n",
    "    print (bin_vars[col])\n",
    "    print (train[bin_vars[col]].unique())\n",
    "    pp = pd.value_counts(train[bin_vars[col]])\n",
    "    \n",
    "    for i in range(0, len(pp)):\n",
    "        if((pp[i]/float(len(train))) <= th):\n",
    "            cols_to_delete.append(bin_vars[col])\n",
    "            \n",
    "print(cols_to_delete)\n",
    "for col in cols_to_delete:\n",
    "    train.drop([col], axis=1, inplace=True)\n",
    "    test.drop([col], axis=1, inplace=True)\n",
    "    \n",
    "cat_cols_to_delete = ['ps_car_07_cat', 'ps_car_10_cat']\n",
    "\n",
    "for col in cat_cols_to_delete:\n",
    "    train.drop([col], axis=1, inplace=True)\n",
    "    test.drop([col], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "other_cols_to_delete = ['ps_ind_14', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_reg_03']\n",
    "for col in other_cols_to_delete:\n",
    "    train.drop([col], axis=1, inplace=True)\n",
    "    test.drop([col], axis=1, inplace=True)\n",
    "    \n",
    "train.to_csv('../../data/train_prepared.csv',index=False)\n",
    "test.to_csv('../../data/test_prepared.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.scatter_matrix(train, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. Remove these variables.\n",
    "\n",
    "For the other categorical variables with missing values, we can leave the missing value -1 as such.\n",
    "\n",
    "ps_reg_03 (continuous) has missing values for 18% of all records. Replace by the mean.\n",
    "\n",
    "ps_car_11 (ordinal) has only 5 records with misisng values. Replace by the mode.\n",
    "\n",
    "ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.\n",
    "\n",
    "ps_car_14(continuous) has missing values for 7% of all records. Replace by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.drop(['ps_car_03_cat', 'ps_car_05_cat'], inplace=True, axis=1)\n",
    "cat_vars.remove('ps_car_03_cat')\n",
    "cat_vars.remove('ps_car_05_cat')\n",
    "\n",
    "print(\"removing features done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Imputing with the mean or mode\n",
    "mean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\n",
    "mode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\n",
    "train['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\n",
    "train['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\n",
    "train['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\n",
    "train['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\n",
    "\n",
    "print(\"Imputing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only categorical variable ps_car_11_cat has a bit more distinct values, although it is still reasonable. \n",
    "#To avoid having many dummy variables later on, we could replace the values \n",
    "#in this variable by the supervised ratio. Other strategies to transform this\n",
    "#variable are explained in an article on KDNuggets. \n",
    "#As a result this variable can then be used as a continuous variable.\n",
    "\n",
    "\n",
    "for f in cat_vars:\n",
    "    dist_values = train[f].value_counts().shape[0]\n",
    "    print('Variable {} has {} distinct values'.format(f, dist_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('../../data/train_prepared.csv',index=False)\n",
    "train.to_csv('../../data/test_prepared.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** @@@@@@@@@@@@@@@@@@@@@@@@@@@ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produza uma matriz de dispersÃ£o para cada um dos pares de atributos dos dados\n",
    "#pd.scatter_matrix(data, alpha = 0.3, figsize = (18,12), diagonal = 'kde');\n",
    "train_target = train_data.pop('target')\n",
    "train_ids = train_data.pop('id')\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "# FormataÃ§Ã£o mais bonita para os notebooks\n",
    "\n",
    "# Nullity or missing values by columns\n",
    "#msno.matrix(df=train_copy.iloc[:,0:30], figsize= (20, 14), color=(0.42, 0.1, 0.05))\n",
    "\n",
    "#msno.matrix(df=train_copy.iloc[:,30:57], figsize=(20, 14), color=(0.42, 0.1, 0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colwithnan = train_copy.columns[train_copy.isnull().any()].tolist()\n",
    "\n",
    "print(\"Just a reminder this dataset has %s Rows. \\n\" % (train_copy.shape[0]))\n",
    "for col in colwithnan:\n",
    "    print(\"Column: %s has %s NaN\" % (col, train_copy[col].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\n",
    "rf.fit(train_data, train_target)\n",
    "features = train_data.columns.values\n",
    "\n",
    "print(\"----- Training Done -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(train_data.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = rf.feature_importances_,\n",
    "    x = features,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 13,\n",
    "        #size= rf.feature_importances_,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = rf.feature_importances_,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = features\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Random Forest Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "     xaxis= dict(\n",
    "         ticklen= 5,\n",
    "         showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False\n",
    "     ),\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), \n",
    "                                                            reverse = False)))\n",
    "trace2 = go.Bar(\n",
    "    x=x ,\n",
    "    y=y,\n",
    "    marker=dict(\n",
    "        color=x,\n",
    "        colorscale = 'Viridis',\n",
    "        reversescale = True\n",
    "    ),\n",
    "    name='Random Forest Feature importance',\n",
    "    orientation='h',\n",
    ")\n",
    "\n",
    "layout = dict(\n",
    "    title='Barplot of Feature importances',\n",
    "     width = 900, height = 2000,\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "#         domain=[0, 0.85],\n",
    "    ))\n",
    "\n",
    "fig1 = go.Figure(data=[trace2])\n",
    "fig1['layout'].update(layout)\n",
    "py.iplot(fig1, filename='plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_feature = []\n",
    "for f in range(28):\n",
    "    important_feature.append(indices[f])\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "print()\n",
    "print(important_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/bertcarremans/data-preparation-exploration\n",
    "\n",
    "https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n",
    "\n",
    "https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "\n",
    "https://github.com/snovik75/porto_seguro/blob/master/notebooks/model.ipynb\n",
    "\n",
    "https://github.com/maksimovkonstantin/KagglePortoSeguro/blob/master/Simple%20model%20creation.ipynb\n",
    "\n",
    "https://github.com/search?p=3&q=porto-seguro&type=Repositories&utf8=%E2%9C%93\n",
    "\n",
    "https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation\n",
    "\n",
    "\n",
    "https://www.kaggle.com/jeru666/easy-to-fork-porto-seguro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
