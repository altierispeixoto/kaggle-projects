{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n"
     ]
    }
   ],
   "source": [
    "# Importar as bibliotecas necessárias para este projeto\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train = pd.read_csv('../../data/train.csv')\n",
    "except e:\n",
    "    print(\"Error on trying read train dataset.\")\n",
    "finally:\n",
    "    print(\"dataset loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 595212 rows and 59 columns\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at how many rows and columns the train dataset contains\n",
    "rows = train.shape[0]\n",
    "columns = train.shape[1]\n",
    "print(\"The train dataset contains {0} rows and {1} columns\".format(rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(train.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "series = pd.Series([train['target'].sum(), len(train.values)], index=['1', '0'], name='train')\n",
    "series.plot.pie(figsize=(7, 7), autopct='%.2f', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 variables of grouping ind\n",
      "There are 3 variables of grouping reg\n",
      "There are 16 variables of grouping car\n",
      "There are 20 variables of grouping calc\n",
      "\n",
      "\n",
      "There are 17 binary variables\n",
      "There are 14 categorical variables\n",
      "There are 28 ordinal/numerical variables\n",
      "\n",
      "\n",
      "So later on we can create dummy variables for the 14 categorical variables.\n",
      "The ordinal/numerical variables we can use as such and the bin variables are already binary.\n"
     ]
    }
   ],
   "source": [
    "ind_vars = [] \n",
    "reg_vars = []\n",
    "car_vars = []\n",
    "calc_vars = []\n",
    "rest_vars = []\n",
    "\n",
    "bin_vars = []\n",
    "cat_vars = []\n",
    "num_ord_vars = []\n",
    "\n",
    "for f in train.columns:\n",
    "    if 'ind' in f:\n",
    "        ind_vars.append(f)\n",
    "    elif 'reg' in f:\n",
    "        reg_vars.append(f)\n",
    "    elif 'car' in f:\n",
    "        car_vars.append(f)\n",
    "    elif 'calc' in f:\n",
    "        calc_vars.append(f)\n",
    "    else:\n",
    "        rest_vars.append(f)\n",
    "        \n",
    "    if 'bin' in f:\n",
    "        bin_vars.append(f)\n",
    "    elif 'cat' in f:\n",
    "        cat_vars.append(f)\n",
    "    else:\n",
    "        num_ord_vars.append(f)\n",
    "        \n",
    "print('There are {} variables of grouping ind'.format(len(ind_vars)))\n",
    "print('There are {} variables of grouping reg'.format(len(reg_vars)))\n",
    "print('There are {} variables of grouping car'.format(len(car_vars)))\n",
    "print('There are {} variables of grouping calc'.format(len(calc_vars)))\n",
    "print('\\n')\n",
    "print('There are {} binary variables'.format(len(bin_vars)))\n",
    "print('There are {} categorical variables'.format(len(cat_vars)))\n",
    "print('There are {} ordinal/numerical variables'.format(len(num_ord_vars)))\n",
    "print(\"\\n\")\n",
    "print(\"So later on we can create dummy variables for the 14 categorical variables.\")\n",
    "print(\"The ordinal/numerical variables we can use as such and the bin variables are already binary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "train_copy = train\n",
    "train_copy = train_copy.replace(-1, np.NaN)\n",
    "\n",
    "# any() applied twice to check run the isnull check across all columns.\n",
    "train_copy.isnull().any().any()\n",
    "msno.bar(train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable ps_ind_02_cat has 216 records (0.04%) with missing values\n",
      "Variable ps_ind_04_cat has 83 records (0.01%) with missing values\n",
      "Variable ps_ind_05_cat has 5809 records (0.98%) with missing values\n",
      "Variable ps_reg_03 has 107772 records (18.11%) with missing values\n",
      "Variable ps_car_01_cat has 107 records (0.02%) with missing values\n",
      "Variable ps_car_02_cat has 5 records (0.00%) with missing values\n",
      "Variable ps_car_03_cat has 411231 records (69.09%) with missing values\n",
      "Variable ps_car_05_cat has 266551 records (44.78%) with missing values\n",
      "Variable ps_car_07_cat has 11489 records (1.93%) with missing values\n",
      "Variable ps_car_09_cat has 569 records (0.10%) with missing values\n",
      "Variable ps_car_11 has 5 records (0.00%) with missing values\n",
      "Variable ps_car_12 has 1 records (0.00%) with missing values\n",
      "Variable ps_car_14 has 42620 records (7.16%) with missing values\n",
      "In total, there are 13 variables with missing values\n"
     ]
    }
   ],
   "source": [
    "vars_with_missing = []\n",
    "\n",
    "for f in train.columns:\n",
    "    missings = train[train[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. Remove these variables.\n",
    "\n",
    "For the other categorical variables with missing values, we can leave the missing value -1 as such.\n",
    "\n",
    "ps_reg_03 (continuous) has missing values for 18% of all records. Replace by the mean.\n",
    "\n",
    "ps_car_11 (ordinal) has only 5 records with misisng values. Replace by the mode.\n",
    "\n",
    "ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.\n",
    "\n",
    "ps_car_14(continuous) has missing values for 7% of all records. Replace by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. Remove these variables.\n",
    "#train.drop(['ps_car_03_cat', 'ps_car_05_cat'], inplace=True, axis=1)\n",
    "#cat_vars.remove('ps_car_03_cat')\n",
    "#cat_vars.remove('ps_car_05_cat')\n",
    "\n",
    "#print(\"removing features done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Imputing with the mean or mode\n",
    "mean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\n",
    "mode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\n",
    "train['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\n",
    "train['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\n",
    "train['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\n",
    "train['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\n",
    "\n",
    "print(\"Imputing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only categorical variable ps_car_11_cat has a bit more distinct values, although it is still reasonable. \n",
    "#To avoid having many dummy variables later on, we could replace the values \n",
    "#in this variable by the supervised ratio. Other strategies to transform this\n",
    "#variable are explained in an article on KDNuggets. \n",
    "#As a result this variable can then be used as a continuous variable.\n",
    "\n",
    "\n",
    "for f in cat_vars:\n",
    "    dist_values = train[f].value_counts().shape[0]\n",
    "    print('Variable {} has {} distinct values'.format(f, dist_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('../../data/train_prepared.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** @@@@@@@@@@@@@@@@@@@@@@@@@@@ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produza uma matriz de dispersão para cada um dos pares de atributos dos dados\n",
    "#pd.scatter_matrix(data, alpha = 0.3, figsize = (18,12), diagonal = 'kde');\n",
    "train_target = train_data.pop('target')\n",
    "train_ids = train_data.pop('id')\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Formatação mais bonita para os notebooks\n",
    "\n",
    "# Nullity or missing values by columns\n",
    "#msno.matrix(df=train_copy.iloc[:,0:30], figsize= (20, 14), color=(0.42, 0.1, 0.05))\n",
    "\n",
    "#msno.matrix(df=train_copy.iloc[:,30:57], figsize=(20, 14), color=(0.42, 0.1, 0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colwithnan = train_copy.columns[train_copy.isnull().any()].tolist()\n",
    "\n",
    "print(\"Just a reminder this dataset has %s Rows. \\n\" % (train_copy.shape[0]))\n",
    "for col in colwithnan:\n",
    "    print(\"Column: %s has %s NaN\" % (col, train_copy[col].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\n",
    "rf.fit(train_data, train_target)\n",
    "features = train_data.columns.values\n",
    "\n",
    "print(\"----- Training Done -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(train_data.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = rf.feature_importances_,\n",
    "    x = features,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 13,\n",
    "        #size= rf.feature_importances_,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = rf.feature_importances_,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = features\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Random Forest Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "     xaxis= dict(\n",
    "         ticklen= 5,\n",
    "         showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False\n",
    "     ),\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), \n",
    "                                                            reverse = False)))\n",
    "trace2 = go.Bar(\n",
    "    x=x ,\n",
    "    y=y,\n",
    "    marker=dict(\n",
    "        color=x,\n",
    "        colorscale = 'Viridis',\n",
    "        reversescale = True\n",
    "    ),\n",
    "    name='Random Forest Feature importance',\n",
    "    orientation='h',\n",
    ")\n",
    "\n",
    "layout = dict(\n",
    "    title='Barplot of Feature importances',\n",
    "     width = 900, height = 2000,\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "#         domain=[0, 0.85],\n",
    "    ))\n",
    "\n",
    "fig1 = go.Figure(data=[trace2])\n",
    "fig1['layout'].update(layout)\n",
    "py.iplot(fig1, filename='plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_feature = []\n",
    "for f in range(28):\n",
    "    important_feature.append(indices[f])\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "print()\n",
    "print(important_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../../data/test.csv')\n",
    "# Êxito\n",
    "print(\"O conjunto de dados de tem {} pontos com {} variáveis em cada.\".format(*test_data.shape))\n",
    "\n",
    "\n",
    "test_ids = test_data.pop('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_enc = OneHotEncoder()\n",
    "\n",
    "one_enc.fit(train_data[vals_types['cat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, train_target, test_size=0.2, random_state=4242)\n",
    "print('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, y_valid)\n",
    "d_test = xgb.DMatrix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set xgboost parameters\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.02\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 4\n",
    "params['subsample'] = 0.9\n",
    "params['colsample_bytree'] = 0.9\n",
    "params['eval_metric'] = 'auc'\n",
    "params['silent'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the data xgboost will test on after eachboosting round\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model! We pass in a max of 10,000 rounds (with early stopping after 100)\n",
    "# and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "\n",
    "\n",
    "print('loading files...')\n",
    "train = pd.read_csv('../../data/train.csv', na_values=-1) \n",
    "test = pd.read_csv('../../data/test.csv', na_values=-1)\n",
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(col_to_drop, axis=1)  \n",
    "test = test.drop(col_to_drop, axis=1)  \n",
    "\n",
    "for c in train.select_dtypes(include=['float64']).columns:\n",
    "    train[c]=train[c].astype(np.float32)\n",
    "    test[c]=test[c].astype(np.float32)\n",
    "for c in train.select_dtypes(include=['int64']).columns[2:]:\n",
    "    train[c]=train[c].astype(np.int8)\n",
    "    test[c]=test[c].astype(np.int8)    \n",
    "\n",
    "print(train.shape, test.shape)\n",
    "    \n",
    "# xgb\n",
    "params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n",
    "\n",
    "X = train.drop(['id', 'target'], axis=1)\n",
    "features = X.columns\n",
    "X = X.values\n",
    "y = train['target'].values\n",
    "sub=test['id'].to_frame()\n",
    "sub['target']=0\n",
    "\n",
    "nrounds=2000  # need to change to 2000\n",
    "kfold = 5  # need to change to 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    d_train = xgb.DMatrix(X_train, y_train) \n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid) \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n",
    "                          feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "    sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values), \n",
    "                        ntree_limit=xgb_model.best_ntree_limit+50) / (2*kfold)\n",
    "    \n",
    "sub.to_csv('submission.csv', index=False, float_format='%.5f')\n",
    "gc.collect()\n",
    "sub.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = xgb_model.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model.best_score\n",
    "\n",
    "#0.258604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ids\n",
    "sub['target'] = p_test\n",
    "sub.to_csv('submission.csv', index=False,float_format='%.5f')\n",
    "\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/bertcarremans/data-preparation-exploration\n",
    "\n",
    "https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n",
    "\n",
    "https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "\n",
    "https://github.com/snovik75/porto_seguro/blob/master/notebooks/model.ipynb\n",
    "\n",
    "https://github.com/maksimovkonstantin/KagglePortoSeguro/blob/master/Simple%20model%20creation.ipynb\n",
    "\n",
    "https://github.com/search?p=3&q=porto-seguro&type=Repositories&utf8=%E2%9C%93\n",
    "\n",
    "https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation\n",
    "\n",
    "\n",
    "https://www.kaggle.com/jeru666/easy-to-fork-porto-seguro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
